{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e94fef3",
   "metadata": {},
   "source": [
    "## Initial Data Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d4e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set reproducibility key for the project\n",
    "SEED = 39\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32292fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f3422d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.175161e-15</td>\n",
       "      <td>3.384974e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.094852e-15</td>\n",
       "      <td>1.021879e-15</td>\n",
       "      <td>1.494498e-15</td>\n",
       "      <td>-5.620335e-16</td>\n",
       "      <td>1.149614e-16</td>\n",
       "      <td>-2.414189e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628620e-16</td>\n",
       "      <td>-3.576577e-16</td>\n",
       "      <td>2.618565e-16</td>\n",
       "      <td>4.473914e-15</td>\n",
       "      <td>5.109395e-16</td>\n",
       "      <td>1.686100e-15</td>\n",
       "      <td>-3.661401e-16</td>\n",
       "      <td>-1.227452e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.175161e-15  3.384974e-16 -1.379537e-15  2.094852e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.021879e-15  1.494498e-15 -5.620335e-16  1.149614e-16 -2.414189e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.628620e-16 -3.576577e-16  2.618565e-16  4.473914e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.109395e-16  1.686100e-15 -3.661401e-16 -1.227452e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50668e2a",
   "metadata": {},
   "source": [
    "- Dataset consists of 31 quantititave variables, with the target being \"Class\". Notably, variables V1, V2, ..., V28 are principle components obtained with PCA already while feature \"Amount\" was kept for interpretation, which might introduce redundant scaling bias into the static model. Standardization for that particular feature might be needed to mitigate this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369a2d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498889c",
   "metadata": {},
   "source": [
    "- Data is efficiently stored in float64 and int64 values, taking up 67.4 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7199b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096ea7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    99.827251\n",
      "1     0.172749\n",
      "Name: proportion, dtype: float64\n",
      "1081\n"
     ]
    }
   ],
   "source": [
    "print(df.Class.value_counts(normalize=True) * 100)\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413d1a9",
   "metadata": {},
   "source": [
    "- As seen from this proportion, the positive events are extremely dominated by the negative events. To address this, methods like class weights and SMOTE might be implemented.\n",
    "- No null values but some duplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cf731",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f4b0c",
   "metadata": {},
   "source": [
    "### Duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487abcb3",
   "metadata": {},
   "source": [
    "- As observed in the Data Diagnosis phase, there are some duplicates in this dataset of credit card transaction. This is common as credit card holders can sometimes swipe twice or encounter errors with the credit card processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cca5b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283726, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.drop(columns=[\"Time\"],inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e82029",
   "metadata": {},
   "source": [
    "- Drop identical duplicates for reduced dataset bias and column \"Time\" to focus on static modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffaa6d",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a5aa2",
   "metadata": {},
   "source": [
    "- When developing a ML model, dedicating partitions of the dataset exclusively for training and testing provides an opportunity to observe how well the model would perform with unseen real data, if deployed in real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Extract the predictors and target variable from the dataset\n",
    "X = df.drop(columns='Class')\n",
    "y = df['Class']\n",
    "\n",
    "# Splitting the training / testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03437055",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bcfc6",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Hypothesis: Removing outliers in the majority class only might improve the PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee04fbd",
   "metadata": {},
   "source": [
    "- No outliers detection or removal was done to preserve the scarce imbalanced data in this dataset. Any datapoint, may it be positive or negative, is crucial to the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88df9e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f9faf",
   "metadata": {},
   "source": [
    "### Stratefied Cross Validation, and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc44e87",
   "metadata": {},
   "source": [
    "- Within the training processes, we further implement cross validation to ensure our trained model is not overfitting or underfitting to our limited training data, this generalization of the model would improve the performance in real data. Stratefied Cross Validation is utilized to preserve class ratio given the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5165f1",
   "metadata": {},
   "source": [
    "- For scoring metrics, we refer to threshold-free metrics like ROC AUC and PR AUC for testing the hypotheses in the Data Preprocessing Phase as the final threshold hasn't been decided yet. Had we chose metrics from the confusion matrix of the default threshold 0.5, we could be completely biased into designing a pipeline that optimizes for the threshold 0.5 only, not all of the possible values.\n",
    "    \n",
    "    - As mentioned, due to the imbalance nature of this problem, a high ROC AUC score can be misleading as a low FPR can still indicate massive false positives due to the dominant negative class. This leads us to the Precision-Recall curve, which put a constraint context on minimizing type 1 and type 2 errors with respect to the true positives, so that the false positives doesn't blow out of proportion.\n",
    "    \n",
    "    - Ideally, we want to maximize PR AUC score in this problem so that during the model selection phase, we can fix a constraint for recall and maximize precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47845dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "\n",
    "# Define Stratified Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=True, \n",
    "                     random_state=SEED)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    'pr_auc': make_scorer(average_precision_score),\n",
    "}\n",
    "\n",
    "# Boilerplate code for displaying metrics\n",
    "def print_results(cv_results):\n",
    "    print(\"Out-of-fold predictions:\")\n",
    "    print(f\"Avg PR AUC: {np.mean(cv_results['test_pr_auc']):.4f} (+/- {np.std(cv_results['test_pr_auc']):.4f})\")\n",
    "\n",
    "    print(\"\\nIn-fold predictions:\")\n",
    "    print(f\"Avg PR AUC: {np.mean(cv_results['train_pr_auc']):.4f} (+/- {np.std(cv_results['train_pr_auc']):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb94da",
   "metadata": {},
   "source": [
    "### Control Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1edb169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold predictions:\n",
      "Avg PR AUC: 0.5278 (+/- 0.0281)\n",
      "\n",
      "In-fold predictions:\n",
      "Avg PR AUC: 0.5453 (+/- 0.0295)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# Define default pipeline\n",
    "pipe_control = Pipeline([\n",
    "    ('model', LogisticRegression(max_iter=10000, # bypass convergence warnings for default configuration\n",
    "                                 penalty=None)) # default value is actually l2 regularization\n",
    "])\n",
    "\n",
    "# Obtain cross validation results\n",
    "cv_results = cross_validate(pipe_control, \n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            cv=cv, \n",
    "                            scoring=scoring_metrics,\n",
    "                            return_train_score=True, # get the in-fold results as well\n",
    "                            n_jobs=-1) # Depends on your machine RAM and number of CPU cores, adjust this number accordingly\n",
    "\n",
    "# Print the results\n",
    "print_results(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9877c",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43785ba",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Hypothesis: To maximize model performance while maintaining the algorithm's computing feasibility, the optimal maximum degree of polynomial is 2.\n",
    "    - Comment: 2 is enough to ensure model complexity is causing overfitting, with 86% average PR AUC score for the training folds.\n",
    "    - Comment: But would it be possible at all to raise the polynomial to 3 and use l1 regularization to reduce the algorithm time complexity?  \n",
    "\n",
    "- Hypothesis: Other scaling methods might outperform standardization for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5758c0f",
   "metadata": {},
   "source": [
    "- Engineering polynomial and interaction features of the dataset increases Logistic Regression complexity, allowing it to learn much more nuanced patterns in the data. However, this comes with a heavy cost. As the maximum degree of polynomials grow, the amount of data expands rapidly, making it unfeasible to compute using personal computers. For this particular project, paired with an AMD Ryzen 9 4900HS with 8 cores and 16GB of RAM, the maximum degree of polynomials is humbly chosen to be 2 so that the algorithm time complexity doesn't grow out of control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d86c86",
   "metadata": {},
   "source": [
    "- Scaling is immediately paired after adding the polynomial features as the fitting algorithm desperately needs the fitting dataset to be standardized, otherwise, your CPU would be exhausted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9bc64",
   "metadata": {},
   "source": [
    "- \"Amount\" is the only feature left for interpretation, thus, we are only adding polynomial terms of principle components in the training dataset and standardize feature \"Amount\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258745d",
   "metadata": {},
   "source": [
    "- Scaling is not enough, we implement PCA in our last step, keeping specified amount of variance only. This is done so that the hyperparameter tuning process can be configured to be computationally feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c115fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Obtain lists of principle component features and amount feature\n",
    "pc_features = X_train.columns.to_list()[:-1]\n",
    "amount_feature = ['Amount']\n",
    "\n",
    "# Build preprocessing pipeline for principle component features\n",
    "pc_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, \n",
    "                                include_bias=False)),\n",
    "    ('scaler_pc', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)) \n",
    "])\n",
    "\n",
    "# Build preprocessing pipeline for interpreting features\n",
    "amount_pipeline = Pipeline([\n",
    "    ('scaler_amount', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the custom column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('pc_processing', \n",
    "         pc_pipeline, \n",
    "         pc_features),\n",
    "        ('interp_processing', \n",
    "         amount_pipeline, \n",
    "         amount_feature)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b43b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold predictions:\n",
      "Avg PR AUC: 0.6131 (+/- 0.0586)\n",
      "\n",
      "In-fold predictions:\n",
      "Avg PR AUC: 0.7429 (+/- 0.0200)\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with polynomials\n",
    "pipe_poly = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=10000,\n",
    "                                 penalty=None))\n",
    "])\n",
    "\n",
    "# Obtain cross validation results\n",
    "cv_results = cross_validate(pipe_poly, \n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            cv=cv, \n",
    "                            scoring=scoring_metrics,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1) # CAREFUL, adjust for your personal computer configuration\n",
    "\n",
    "# Print the results\n",
    "print_results(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4b379",
   "metadata": {},
   "source": [
    "- As expected, adding polynomial features increases the model complexity but it's also prone to overfitting. Regularization is needed to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047ddb9",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c8bcb",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Hypothesis: Would any configuration of these imbalance methods improve the PR AUC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f974089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold predictions:\n",
      "Avg PR AUC: 0.6218 (+/- 0.0504)\n",
      "\n",
      "In-fold predictions:\n",
      "Avg PR AUC: 0.7555 (+/- 0.0181)\n"
     ]
    }
   ],
   "source": [
    "# Define model pipeline with class-weighting as balanced\n",
    "pipe_class_weight = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=10000,\n",
    "                                 penalty=None,\n",
    "                                 class_weight={0:1, 1:2}))\n",
    "])\n",
    "\n",
    "# Obtain cross validation results\n",
    "cv_results = cross_validate(pipe_class_weight, \n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            cv=cv, \n",
    "                            scoring=scoring_metrics,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1) # CAREFUL, adjust for your personal computer configuration\n",
    "\n",
    "# Print the results\n",
    "print_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold predictions:\n",
      "Avg PR AUC: 0.0495 (+/- 0.0019)\n",
      "\n",
      "In-fold predictions:\n",
      "Avg PR AUC: 0.0516 (+/- 0.0034)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as imbPipeline \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define model pipeline with SMOTE\n",
    "pipe_smote = imbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTE(random_state=SEED)), \n",
    "    ('model', LogisticRegression(max_iter=1000,\n",
    "                                 penalty=None))\n",
    "])\n",
    "\n",
    "# Obtain cross validation results\n",
    "cv_results = cross_validate(pipe_smote, \n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            cv=cv, \n",
    "                            scoring=scoring_metrics,\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1) # CAREFUL, adjust for your personal computer configuration\n",
    "\n",
    "# Print the results\n",
    "print_results(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef6e11",
   "metadata": {},
   "source": [
    "- As seen from these results, implementing imbalance methods defaultly, like Class-Weighting and SMOTE, aren't effective for maximizing PR AUC for this Fraud Detection System. The main reason being that these methods improve the ROC AUC but also simultaneously flag a lot of nearby negatives to be positive as well, hence the abundance of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68568f",
   "metadata": {},
   "source": [
    "- New discovery! Class-weighting with ratio 2:1 for positive class seems to slightly improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627e9c8",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef13817",
   "metadata": {},
   "source": [
    "### Optimal Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ecb30",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Illustrate which solver is the best in this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d738918",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edc850",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Hypothesis: Which regularization method is the most effective?\n",
    "- Interpret the code\n",
    "- Interpret the first algorithm iteration results before modifying the parameter grid for better estimate\n",
    "- No idea how to feasibly find the optimal hyperparameters T.T\n",
    "    - Should elasticnet even be considered? any other choice of regularization that doesn't take as much computing time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d94b6",
   "metadata": {},
   "source": [
    "- We don't quite know which method of regularization would work better so ElasticNet would be a good initial choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d411ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print grid search results\n",
    "def print_grid_search_results(gs_results):\n",
    "    best_index = gs_results.best_index_\n",
    "    cv_results = gs_results.cv_results_\n",
    "    print(\"Out-of-fold predictions (for best parameter):\")\n",
    "    for scorer in gs_results.scorer_:\n",
    "        print(f\"Avg PR AUC: \\t{cv_results[f'mean_test_{scorer}'][best_index]:.4f} (+/- {cv_results[f'std_test_{scorer}'][best_index]:.4f})\")\n",
    "\n",
    "    print(\"\\nIn-fold predictions (for best parameter):\")\n",
    "    for scorer in gs_results.scorer_:\n",
    "        print(f\"Avg PR AUC: \\t{cv_results[f'mean_train_{scorer}'][best_index]:.4f} (+/- {cv_results[f'std_train_{scorer}'][best_index]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ec92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model pipeline with elastic net regularization\n",
    "pipe_elasticnet = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, \n",
    "                                include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000,\n",
    "                                 penalty='elasticnet',\n",
    "                                 solver='saga', # only saga solver is eligible for elasticnet regularization\n",
    "                                 class_weight={0:1, 1:0.5}))\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'model__l1_ratio': [0.1, 0.5, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_elasticnet,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='pr_auc',\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=7 # CAREFUL, adjust for your personal computer configuration\n",
    ")\n",
    "\n",
    "# Run the grid search algorithm\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best C parameter: {grid_search.best_params_}\")\n",
    "print(f\"Best PR AUC score: {grid_search.best_score_:.4f}\")\n",
    "print()\n",
    "\n",
    "print_grid_search_results(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f0a37",
   "metadata": {},
   "source": [
    "- It seems that for Logistic Regression, this value of PR AUC of 64.5% is approaching the upper limit of the model capabilities. We can finalize the model by finding the best threshold next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfa545",
   "metadata": {},
   "source": [
    "### Threshold Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c9597",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Interpret the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a88b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold (from CV): 0.0176\n",
      "Max F3-Score (on CV):    0.8248\n",
      "Precision at Threshold (on CV): 0.6414\n",
      "Recall at Threshold (on CV):  0.8519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "\n",
    "\n",
    "BETA = 3  # Adjust this constant to emphasize recall better\n",
    "\n",
    "best_c = LogisticRegression(max_iter=1000,\n",
    "                            penalty='l2',\n",
    "                            C=0.01)\n",
    "\n",
    "# Obtain cross-validated probabilities (out-of-fold)\n",
    "y_cv_scores = cross_val_predict(best_c, \n",
    "                                X_train, \n",
    "                                y_train, \n",
    "                                cv=cv,\n",
    "                                method=\"predict_proba\")[:,1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_cv_scores)\n",
    "# Exclude last point (recall reaches 1.0 with no threshold)\n",
    "p = precision[:-1]\n",
    "r = recall[:-1]\n",
    "\n",
    "fbeta_scores = (1 + BETA**2) * (p * r) / ((BETA**2 * p) + r)\n",
    "fbeta_scores = np.nan_to_num(fbeta_scores, nan=0.0)\n",
    "\n",
    "best_idx = np.argmax(fbeta_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_fbeta = fbeta_scores[best_idx]\n",
    "best_precision = p[best_idx]\n",
    "best_recall = r[best_idx]\n",
    "\n",
    "print(f\"Optimal Threshold (from CV): {best_threshold:.4f}\")\n",
    "print(f\"Max F{BETA}-Score (on CV):    {best_fbeta:.4f}\")\n",
    "print(f\"Precision at Threshold (on CV): {best_precision:.4f}\")\n",
    "print(f\"Recall at Threshold (on CV):  {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaafc28",
   "metadata": {},
   "source": [
    "- From many trials, the PR curve seems to plateau at 85% recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37961835",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc482d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# Classification Report\n",
    "\n",
    "# PR AUC (Average Precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bbf95",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "666d08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "\n",
    "\n",
    "# Plot Precision-Recall Curve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditCardFraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
